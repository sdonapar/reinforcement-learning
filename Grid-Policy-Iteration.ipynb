{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 (Dynamic Programming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self,n,terminal_states,discount_factor=1,r=-1,actions=('left','right','top','down'),delta=0.0005):\n",
    "        \"\"\"\n",
    "        This class creates n x n grid\n",
    "        Four actions allowed Left, Right, Top and Bottom at any state\n",
    "        This uses a random policy, equi-probable\n",
    "        Any action results in a reward of -1\n",
    "        if the actions takes out the grid, then it remains in the state\n",
    "        You can define the terminal states in the grid\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.terminal_states = terminal_states # tupples of row and column\n",
    "        self.df = discount_factor\n",
    "        # Initialize the values\n",
    "        self.row = 0\n",
    "        self.col = 0\n",
    "        self.delta = delta\n",
    "        self.VC = np.zeros((n,n))\n",
    "        self.VN = np.zeros((n,n))\n",
    "        self.V = [self.VC]\n",
    "        self.Q = [self.VC]\n",
    "        self.iteration = 0\n",
    "        self.reward = {}\n",
    "        self.actions= actions\n",
    "        self.states = []\n",
    "        self.policies = []\n",
    "        self.converge = False\n",
    "        self.diff = None\n",
    "        self.found_optimal_policy = False\n",
    "        \n",
    "        self.initialize_states()\n",
    "        self.create_random_policy()\n",
    "        self.create_reward_function()\n",
    "        \n",
    "    def set_state(self,state):\n",
    "        self.row = state[0]\n",
    "        self.col = state[1]\n",
    "        \n",
    "    def initialize_states(self):\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                self.states.append((i,j))\n",
    "        \n",
    "    def create_random_policy(self):\n",
    "        random_policy = {}\n",
    "        for state in self.states:\n",
    "            if state not in self.terminal_states:\n",
    "                random_policy[state] = {}\n",
    "                for action in self.actions:\n",
    "                    random_policy[state][action] = 1.0/len(self.actions)\n",
    "        self.policies.append(random_policy)\n",
    "    def create_reward_function(self):\n",
    "        for state in self.states:\n",
    "            self.set_state(state)\n",
    "            for action in self.actions:\n",
    "                target_state = self.get_sp(action)\n",
    "                self.reward[state,action,target_state] = -1\n",
    "    def get_sp(self,action):\n",
    "        \"\"\"\n",
    "        This function returns the sp ( s-prime) based on current position and action\n",
    "        \"\"\"\n",
    "        i_s = self.row\n",
    "        j_s = self.col\n",
    "        i_sp = i_s\n",
    "        j_sp = j_s\n",
    "        if action == \"left\":\n",
    "            j_sp = j_s - 1\n",
    "        if action == \"right\":\n",
    "            j_sp = j_s + 1\n",
    "        if action == \"top\":\n",
    "            i_sp = i_s - 1\n",
    "        if action==\"down\":\n",
    "            i_sp = i_s + 1\n",
    "        if i_sp < 0 or i_sp > self.n - 1:\n",
    "            i_sp = i_s\n",
    "        if j_sp < 0 or j_sp > self.n - 1:\n",
    "            j_sp = j_s\n",
    "        return i_sp,j_sp\n",
    "\n",
    "    def calculate_state_value(self):\n",
    "        \"\"\"\n",
    "        This function calcuates the state value and update the same\n",
    "        \"\"\"\n",
    "        i_s = self.row\n",
    "        j_s = self.col\n",
    "        \n",
    "        #latest optimal policy\n",
    "        policy = self.policies[-1]\n",
    "        \n",
    "        if (i_s,j_s) in self.terminal_states:\n",
    "            self.VN[i_s,j_s] = 0\n",
    "        else:\n",
    "            new_value = 0\n",
    "            for action,p in policy[(i_s,j_s)].items():\n",
    "                i_sp, j_sp = self.get_sp(action)\n",
    "                r = self.reward.get(((i_s,j_s),action,(i_sp,j_sp)),0)\n",
    "                new_value += p*(r+self.df*self.VC[i_sp,j_sp])\n",
    "            self.VN[i_s,j_s] = new_value\n",
    "    \n",
    "    def update_state_values(self):\n",
    "        for state in self.states:\n",
    "            if not state in self.terminal_states:\n",
    "                self.set_state(state)\n",
    "                self.calculate_state_value()\n",
    "        self.iteration += 1\n",
    "        self.VC = np.array(self.VN)\n",
    "        self.V.append(self.VC)      \n",
    "\n",
    "    def check_convergence(self):\n",
    "        n = len(self.V)\n",
    "        if n > 2:\n",
    "            diff = np.abs((self.V[n-1]-self.V[n-2]).flatten())\n",
    "            converge = (diff <= self.delta).all()\n",
    "            self.converge = converge\n",
    "            self.diff = diff\n",
    "    \n",
    "    def evaluate_policy(self,delta=None,k=None):\n",
    "        if delta:\n",
    "            self.delta = delta\n",
    "        if k is None:\n",
    "            while not self.converge:\n",
    "                self.update_state_values()\n",
    "                self.check_convergence()\n",
    "            print(f\"Converged at {self.iteration}\")\n",
    "            print(self.diff)\n",
    "        else:\n",
    "            for x in range(k):\n",
    "                self.update_state_values()\n",
    "                self.check_convergence()\n",
    "            print(self.converge)\n",
    "            print(self.diff)\n",
    "            \n",
    "    def improve_policy(self):\n",
    "        new_policy = {}\n",
    "        policy = self.policies[-1]\n",
    "        q_values = np.array(self.Q[-1])\n",
    "        for state in self.states:\n",
    "            if not state in self.terminal_states:\n",
    "                new_policy[state] = {}\n",
    "                actions = []\n",
    "                q = []\n",
    "                for action,p in policy[state].items():\n",
    "                    actions.append(action)\n",
    "                    self.set_state(state)\n",
    "                    target_state = grid.get_sp(action)\n",
    "                    prob = policy.get(state,{}).get(action,0)\n",
    "                    reward = self.reward.get((state,action,target_state),0)\n",
    "                    qa = prob*(reward+grid.df*grid.VC[target_state])\n",
    "                    q.append(qa)\n",
    "                q_max = np.max(q)\n",
    "                b = [True if np.round(q_val,4)==np.round(q_max,4) else False for q_val in q]\n",
    "                optimal_actions = np.array(actions)[b]\n",
    "                q_values[state] = q_max\n",
    "                for o_action in optimal_actions:\n",
    "                    new_policy[state][o_action] = 1.0/len(optimal_actions)\n",
    "        self.Q.append(q_values)\n",
    "        self.policies.append(new_policy)\n",
    "        self.found_optimal_policy =  self.policies[-2] == self.policies[-1]\n",
    "        \n",
    "    def find_optimal_policy(self):\n",
    "        while not self.found_optimal_policy:\n",
    "            self.evaluate_policy()\n",
    "            self.improve_policy()\n",
    "        print(self.policies[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid(4,terminal_states=[(3,3)],delta=0.0005)\n",
    "#grid.iterate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at 391\n",
      "[ 0.00049027  0.00047085  0.0004408   0.00041673  0.00047085  0.00044417\n",
      "  0.00039988  0.00035964  0.0004408   0.00039988  0.00032325  0.0002338\n",
      "  0.00041673  0.00035964  0.0002338   0.        ]\n",
      "Converged at 391\n",
      "[ 0.00049027  0.00047085  0.0004408   0.00041673  0.00047085  0.00044417\n",
      "  0.00039988  0.00035964  0.0004408   0.00039988  0.00032325  0.0002338\n",
      "  0.00041673  0.00035964  0.0002338   0.        ]\n",
      "{(0, 0): {'right': 0.5, 'down': 0.5}, (0, 1): {'right': 1.0}, (0, 2): {'down': 1.0}, (0, 3): {'down': 1.0}, (1, 0): {'down': 1.0}, (1, 1): {'right': 0.5, 'down': 0.5}, (1, 2): {'down': 1.0}, (1, 3): {'down': 1.0}, (2, 0): {'right': 1.0}, (2, 1): {'right': 1.0}, (2, 2): {'right': 0.5, 'down': 0.5}, (2, 3): {'down': 1.0}, (3, 0): {'right': 1.0}, (3, 1): {'right': 1.0}, (3, 2): {'right': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "grid.find_optimal_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) {'right': 0.5, 'down': 0.5}\n",
      "(0, 1) {'right': 1.0}\n",
      "(0, 2) {'down': 1.0}\n",
      "(0, 3) {'down': 1.0}\n",
      "(1, 0) {'down': 1.0}\n",
      "(1, 1) {'right': 0.5, 'down': 0.5}\n",
      "(1, 2) {'down': 1.0}\n",
      "(1, 3) {'down': 1.0}\n",
      "(2, 0) {'right': 1.0}\n",
      "(2, 1) {'right': 1.0}\n",
      "(2, 2) {'right': 0.5, 'down': 0.5}\n",
      "(2, 3) {'down': 1.0}\n",
      "(3, 0) {'right': 1.0}\n",
      "(3, 1) {'right': 1.0}\n",
      "(3, 2) {'right': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for state,action in grid.policies[-1].items():\n",
    "    print(state,action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ V(s) = \\sum \\pi(s)\\sum p(s',r|s,a) (r + \\gamma V(s')$    $\\forall a \\in  A(s), s,s' \\in S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r(s,a,s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(s,a,s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q(s,a) = \\sum p(s',r|s,a) (r + \\gamma V(s')$    $\\forall a \\in  A(s), s,s' \\in S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
